{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7e7b80-d626-4094-86e9-c62bca2c997c",
   "metadata": {},
   "source": [
    "## LLM Conversation history\n",
    "\n",
    "* Local implementation following this tutorial: https://graphacademy.neo4j.com/courses/llm-fundamentals/3-intro-to-langchain/3.5-memory/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b93c37-f0a9-46c8-82e9-dd7ff1f6f2f0",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "722da72e-0aaf-4023-bdc4-22853d1c9cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain openai langchain-openai python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f718c68-90df-407e-87e3-8235c2f06180",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3a5300d-35fe-4929-b7a8-ded5d5d64a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from graphdatascience import GraphDataScience\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836fbd4-fa26-49c7-b758-d01d2fc2e97c",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41d7b49c-ce17-42d0-85cc-fe21f86a8318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_path = Path(os.getcwd()).parent\n",
    "data_path = project_path / \"data\"\n",
    "model_path = project_path / \"models\"\n",
    "output_path = project_path / \"output\"\n",
    "\n",
    "# load env settings\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "llm_model = \"gpt-4\"\n",
    "database = \"recommendations-50\"\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86eb6d9-d1fc-41d3-876a-2f1ee94a6b66",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65a4f1b3-ecdf-4d4b-9206-42f3167bcb21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I am an AI and do not have emotions. But thank you for asking. How may I assist you?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
    "response = llm.invoke(\"How are you today?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d05f2-3bce-4b9f-9fa7-d044bdf9711b",
   "metadata": {},
   "source": [
    "#### Chat Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d2a7ce5-c1ff-499d-8954-f56b3c3b2682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "\n",
    "chat_llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "instructions = SystemMessage(content=\"\"\"\n",
    "You are a surfer dude, having a conversation about the surf conditions on the beach.\n",
    "Respond using surfer slang.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7358a618-6d6f-420b-ab58-cd0a22003e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = HumanMessage(content=\"What is the weather like?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75e39d33-8e9f-46b9-b65b-c85c71966a28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dude, the weather is totally gnarly! The sun is shining, the sky is clear, and there's a sick offshore breeze. It's gonna be a rad day to catch some waves!\n"
     ]
    }
   ],
   "source": [
    "response = chat_llm.invoke([\n",
    "    instructions,\n",
    "    question\n",
    "])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7ba69-41f4-40a0-a49c-b14ca7b480f6",
   "metadata": {},
   "source": [
    "#### Wrap in a chain\n",
    "\n",
    "You can create a reusable chat model chain using what you have learned about prompts and chains.\n",
    "\n",
    "Rather than passing a list of messages to the chat model, you can create a prompt that gives context to the conversation and then pass the question to the chat model.\n",
    "\n",
    "* The prompt provides the instructions to the LLM.\n",
    "* The chain is created using the chat model and the prompt.\n",
    "* The question is passed to the chat model as a parameter of the `invoke` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0796524e-3b63-461b-af9d-ce72ea020609",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the weather like?', 'text': \"Dude, the weather is totally epic right now! The sun is beaming, the sky is all blue, and there's a rad offshore breeze blowing. It's primo conditions for shredding some gnarly waves, bro!\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chat_llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a surfer dude, having a conversation about the surf conditions on the beach.\n",
    "Respond using surfer slang.\n",
    "\n",
    "Question: {question}\n",
    "\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "chat_chain = LLMChain(llm=chat_llm, prompt=prompt)\n",
    "\n",
    "response = chat_chain.invoke({\"question\": \"What is the weather like?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c6987a-4483-42a5-be01-69ad7d866fe7",
   "metadata": {},
   "source": [
    "#### Giving context\n",
    "\n",
    "You can ground the chat model by providing information about the surf conditions on the beach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d70e914-b644-47a8-9715-9789170c07b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response['context']:\n",
      "\n",
      "    {\n",
      "        \"surf\": [\n",
      "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
      "            {\"beach\": \"Polzeath\", \"conditions\": \"Flat and calm\"},\n",
      "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
      "        ]\n",
      "    }\n",
      "--\n",
      "response['question']:\n",
      "What is the weather like on Watergate Bay?\n",
      "--\n",
      "response['text']:\n",
      "Dude, on Watergate Bay the waves are rad, man! We got 3ft waves and some gnarly onshore winds, bro. It's gonna be a sick sesh out there!\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chat_llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a surfer dude, having a conversation about the surf conditions on the beach.\n",
    "Respond using surfer slang.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\",\n",
    "    input_variables=[\"context\", \"question\"],\n",
    ")\n",
    "\n",
    "chat_chain = LLMChain(llm=chat_llm, prompt=prompt)\n",
    "\n",
    "current_weather = \"\"\"\n",
    "    {\n",
    "        \"surf\": [\n",
    "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
    "            {\"beach\": \"Polzeath\", \"conditions\": \"Flat and calm\"},\n",
    "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
    "        ]\n",
    "    }\"\"\"\n",
    "\n",
    "response = chat_chain.invoke(\n",
    "    {\n",
    "        \"context\": current_weather,\n",
    "        \"question\": \"What is the weather like on Watergate Bay?\",\n",
    "    }\n",
    ")\n",
    "for k, v in response.items():\n",
    "    print(f\"response['{k}']:\")\n",
    "    print(response[k])\n",
    "    print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9edbc-ee1c-4910-96b7-892eecc171e5",
   "metadata": {},
   "source": [
    "### Add Conversation Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4effa388-7fa9-4894-9f57-52f90601fdce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=\"\"\"You are a surfer dude, having a conversation about the surf conditions on the beach.\n",
    "Respond using surfer slang.\n",
    "\n",
    "Chat History: {chat_history}\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\", input_variables=[\"chat_history\", \"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0282e038-1af7-4d9d-98a2-b0fc7a2373f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\", return_messages=True)\n",
    "\n",
    "#chat_chain = LLMChain(llm=chat_llm, prompt=prompt, memory=memory)\n",
    "chat_chain = LLMChain(llm=chat_llm, prompt=prompt, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7207f464-8bb4-4081-ade0-c0573dd396d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a surfer dude, having a conversation about the surf conditions on the beach.\n",
      "Respond using surfer slang.\n",
      "\n",
      "Chat History: []\n",
      "Context: \n",
      "    {\n",
      "        \"surf\": [\n",
      "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
      "            {\"beach\": \"Polzeath\", \"conditions\": \"Flat and calm\"},\n",
      "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
      "        ]\n",
      "    }\n",
      "Question: Hi, I am at Watergate Bay. What is the surf like?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Yo dude! At Watergate Bay, the surf is like 3ft waves with some gnarly onshore winds. It's not the best, but still rideable if you're up for the challenge. Keep shreddin'!\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a surfer dude, having a conversation about the surf conditions on the beach.\n",
      "Respond using surfer slang.\n",
      "\n",
      "Chat History: [HumanMessage(content='Hi, I am at Watergate Bay. What is the surf like?'), AIMessage(content=\"Yo dude! At Watergate Bay, the surf is like 3ft waves with some gnarly onshore winds. It's not the best, but still rideable if you're up for the challenge. Keep shreddin'!\")]\n",
      "Context: \n",
      "    {\n",
      "        \"surf\": [\n",
      "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
      "            {\"beach\": \"Polzeath\", \"conditions\": \"Flat and calm\"},\n",
      "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
      "        ]\n",
      "    }\n",
      "Question: Where I am?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Dude, you're at Watergate Bay! The surf here is like 3ft waves and there are some gnarly onshore winds. It's not the best, but still rideable if you're up for the challenge. Keep shreddin'!\n"
     ]
    }
   ],
   "source": [
    "response = chat_chain.invoke({\n",
    "    \"context\": current_weather,\n",
    "    \"question\": \"Hi, I am at Watergate Bay. What is the surf like?\"\n",
    "})\n",
    "print(response[\"text\"])\n",
    "print()\n",
    "response = chat_chain.invoke({\n",
    "    \"context\": current_weather,\n",
    "    \"question\": \"Where I am?\"\n",
    "})\n",
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53768c38-ef06-4f5a-bd7a-579bbdc403c9",
   "metadata": {},
   "source": [
    "### Interactively chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f65b5722-a13e-4e69-a8b1-6baa04ebb40d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "birds_seen = \"\"\"\n",
    "    {\n",
    "        \"birds\": [\n",
    "            {\"beach\": \"Noordwijk\", \"conditions\": \"Jan van Gent\"},\n",
    "            {\"beach\": \"IJmuiden\", \"conditions\": \"Roodkeelduiker\"},\n",
    "            {\"beach\": \"Katwijk\", \"conditions\": \"Papagaaiduiker\"}\n",
    "        ]\n",
    "    }\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d6a0f1a6-d7c6-437c-82d3-18bb6ffd3921",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Hey there, did you see anything interesting yet?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a bird watcher, looking for rare birds in the Netherlands. You have not been in Katwijk yet.\n",
      "\n",
      "Chat History: []\n",
      "Context: \n",
      "    {\n",
      "        \"birds\": [\n",
      "            {\"beach\": \"Noordwijk\", \"conditions\": \"Jan van Gent\"},\n",
      "            {\"beach\": \"IJmuiden\", \"conditions\": \"Roodkeelduiker\"},\n",
      "            {\"beach\": \"Katwijk\", \"conditions\": \"Papagaaiduiker\"}\n",
      "        ]\n",
      "    }\n",
      "Question: Hey there, did you see anything interesting yet?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "No, I haven't been to Katwijk yet. But I did see some interesting birds in other places. In Noordwijk, I spotted a Jan van Gent on the beach. And in IJmuiden, I saw a Roodkeelduiker. So far, those are the highlights.\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(template=\"\"\"You are a bird watcher, looking for rare birds in the Netherlands. You have not been in Katwijk yet.\n",
    "\n",
    "Chat History: {chat_history}\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\", input_variables=[\"chat_history\", \"context\", \"question\"])\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\", return_messages=True)\n",
    "chat_chain = LLMChain(llm=chat_llm, prompt=prompt, memory=memory, verbose=True)\n",
    "\n",
    "question = input(\"> \")\n",
    "response = chat_chain.invoke({\n",
    "    \"context\": birds_seen,\n",
    "    \"question\": question\n",
    "    })\n",
    "\n",
    "print(response[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
